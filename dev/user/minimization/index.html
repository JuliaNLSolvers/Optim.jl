<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Minimizing a function · Optim</title><meta name="title" content="Minimizing a function · Optim"/><meta property="og:title" content="Minimizing a function · Optim"/><meta property="twitter:title" content="Minimizing a function · Optim"/><meta name="description" content="Documentation for Optim."/><meta property="og:description" content="Documentation for Optim."/><meta property="twitter:description" content="Documentation for Optim."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Optim</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Tutorials</span><ul><li class="is-active"><a class="tocitem" href>Minimizing a function</a><ul class="internal"><li><a class="tocitem" href="#Unconstrained-Optimization"><span>Unconstrained Optimization</span></a></li><li><a class="tocitem" href="#Box-Constrained-Optimization"><span>Box Constrained Optimization</span></a></li><li><a class="tocitem" href="#Minimizing-a-univariate-function-on-a-bounded-interval"><span>Minimizing a univariate function on a bounded interval</span></a></li><li><a class="tocitem" href="#Obtaining-results"><span>Obtaining results</span></a></li><li><a class="tocitem" href="#Input-types"><span>Input types</span></a></li><li><a class="tocitem" href="#Notes-on-convergence-flags-and-checks"><span>Notes on convergence flags and checks</span></a></li></ul></li><li><a class="tocitem" href="../gradientsandhessians/">Gradients and Hessians</a></li><li><a class="tocitem" href="../config/">Configurable Options</a></li><li><a class="tocitem" href="../../algo/linesearch/">Linesearch</a></li><li><a class="tocitem" href="../algochoice/">Algorithm choice</a></li><li><a class="tocitem" href="../../algo/precondition/">Preconditioners</a></li><li><a class="tocitem" href="../../algo/complex/">Complex optimization</a></li><li><a class="tocitem" href="../../algo/manifolds/">Manifolds</a></li><li><a class="tocitem" href="../tipsandtricks/">Tips and tricks</a></li><li><a class="tocitem" href="../../examples/generated/ipnewton_basics/">Interior point Newton</a></li><li><a class="tocitem" href="../../examples/generated/maxlikenlm/">Maximum likelihood estimation</a></li><li><a class="tocitem" href="../../examples/generated/rasch/">Conditional maximum likelihood estimation</a></li></ul></li><li><span class="tocitem">Algorithms</span><ul><li><input class="collapse-toggle" id="menuitem-3-1" type="checkbox"/><label class="tocitem" for="menuitem-3-1"><span class="docs-label">Gradient Free</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../algo/nelder_mead/">Nelder Mead</a></li><li><a class="tocitem" href="../../algo/simulated_annealing/">Simulated Annealing</a></li><li><a class="tocitem" href="../../algo/samin/">Simulated Annealing w/ bounds</a></li><li><a class="tocitem" href="../../algo/particle_swarm/">Particle Swarm</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-3-2" type="checkbox"/><label class="tocitem" for="menuitem-3-2"><span class="docs-label">Gradient Required</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../algo/adam_adamax/">Adam and AdaMax</a></li><li><a class="tocitem" href="../../algo/cg/">Conjugate Gradient</a></li><li><a class="tocitem" href="../../algo/gradientdescent/">Gradient Descent</a></li><li><a class="tocitem" href="../../algo/lbfgs/">(L-)BFGS</a></li><li><a class="tocitem" href="../../algo/ngmres/">Acceleration</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-3-3" type="checkbox"/><label class="tocitem" for="menuitem-3-3"><span class="docs-label">Hessian Required</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../algo/newton/">Newton</a></li><li><a class="tocitem" href="../../algo/newton_trust_region/">Newton with Trust Region</a></li><li><a class="tocitem" href="../../algo/ipnewton/">Interior point Newton</a></li></ul></li></ul></li><li><a class="tocitem" href="../../dev/contributing/">Contributing</a></li><li><a class="tocitem" href="../../LICENSE/">License</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorials</a></li><li class="is-active"><a href>Minimizing a function</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Minimizing a function</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaNLSolvers/Optim.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JuliaNLSolvers/Optim.jl/blob/master/docs/src/user/minimization.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h2 id="Unconstrained-Optimization"><a class="docs-heading-anchor" href="#Unconstrained-Optimization">Unconstrained Optimization</a><a id="Unconstrained-Optimization-1"></a><a class="docs-heading-anchor-permalink" href="#Unconstrained-Optimization" title="Permalink"></a></h2><p>To show how the Optim package can be used, we minimize the <a href="http://en.wikipedia.org/wiki/Rosenbrock_function">Rosenbrock function</a>, a classical test problem for numerical optimization. We&#39;ll assume that you&#39;ve already installed the Optim package using Julia&#39;s package manager. First, we load Optim and define the Rosenbrock function:</p><pre><code class="language-jl hljs">using Optim
f(x) = (1.0 - x[1])^2 + 100.0 * (x[2] - x[1]^2)^2</code></pre><p>Once we&#39;ve defined this function, we can find the minimizer (the input that minimizes the objective) and the minimum (the value of the objective at the minimizer) using any of our favorite optimization algorithms. With a function defined, we just specify an initial point <code>x</code> and call <code>optimize</code> with a starting point <code>x0</code>:</p><pre><code class="language-jl hljs">x0 = [0.0, 0.0]
optimize(f, x0)</code></pre><p><em>Note</em>: it is important to pass <code>initial_x</code> as an array. If your problem is one-dimensional, you have to wrap it in an array. An easy way to do so is to write <code>optimize(x-&gt;f(first(x)), [initial_x])</code> which make sure the input is an array, but the anonymous function automatically passes the first (and only) element onto your given <code>f</code>.</p><p>Optim will default to using the Nelder-Mead method in the multivariate case, as we did not provide a gradient. This can also be explicitly specified using:</p><pre><code class="language-jl hljs">optimize(f, x0, NelderMead())</code></pre><p>Other solvers are available. Below, we use L-BFGS, a quasi-Newton method that requires a gradient. If we pass <code>f</code> alone, Optim will construct an approximate gradient for us using central finite differencing:</p><pre><code class="language-jl hljs">optimize(f, x0, LBFGS())</code></pre><p>For better performance and greater precision, you can pass your own gradient function. If your objective is written in all Julia code with no special calls to external (that is non-Julia) libraries, you can also use automatic differentiation, by using the <code>autodiff</code> keyword and setting it to <code>:forward</code>:</p><pre><code class="language-julia hljs">optimize(f, x0, LBFGS(); autodiff = :forward)</code></pre><p>For the Rosenbrock example, the analytical gradient can be shown to be:</p><pre><code class="language-jl hljs">function g!(G, x)
    G[1] = -2.0 * (1.0 - x[1]) - 400.0 * (x[2] - x[1]^2) * x[1]
    G[2] = 200.0 * (x[2] - x[1]^2)
end</code></pre><p>Note, that the functions we&#39;re using to calculate the gradient (and later the Hessian <code>h!</code>) of the Rosenbrock function mutate a fixed-sized storage array, which is passed as an additional argument called <code>G</code> (or <code>H</code> for the Hessian) in these examples. By mutating a single array over many iterations, this style of function definition removes the sometimes considerable costs associated with allocating a new array during each call to the <code>g!</code> or <code>h!</code> functions. If you prefer to have your gradients simply accept an <code>x</code>, you can still use <code>optimize</code> by setting the <code>inplace</code> keyword to <code>false</code>:</p><pre><code class="language-jl hljs">optimize(f, g, x0; inplace = false)</code></pre><p>where <code>g</code> is a function of <code>x</code> only.</p><p>Returning to our in-place version, you simply pass <code>g!</code> together with <code>f</code> from before to use the gradient:</p><pre><code class="language-jl hljs">optimize(f, g!, x0, LBFGS())</code></pre><p>For some methods, like simulated annealing, the gradient will be ignored:</p><pre><code class="language-jl hljs">optimize(f, g!, x0, SimulatedAnnealing())</code></pre><p>In addition to providing gradients, you can provide a Hessian function <code>h!</code> as well. In our current case this is:</p><pre><code class="language-jl hljs">function h!(H, x)
    H[1, 1] = 2.0 - 400.0 * x[2] + 1200.0 * x[1]^2
    H[1, 2] = -400.0 * x[1]
    H[2, 1] = -400.0 * x[1]
    H[2, 2] = 200.0
end</code></pre><p>Now we can use Newton&#39;s method for optimization by running:</p><pre><code class="language-jl hljs">optimize(f, g!, h!, x0)</code></pre><p>Which defaults to <code>Newton()</code> since a Hessian function was provided. Like gradients, the Hessian function will be ignored if you use a method that does not require it:</p><pre><code class="language-jl hljs">optimize(f, g!, h!, x0, LBFGS())</code></pre><p>Note that Optim will not generate approximate Hessians using finite differencing because of the potentially low accuracy of approximations to the Hessians. Other than Newton&#39;s method, none of the algorithms provided by the Optim package employ exact Hessians.</p><h2 id="Box-Constrained-Optimization"><a class="docs-heading-anchor" href="#Box-Constrained-Optimization">Box Constrained Optimization</a><a id="Box-Constrained-Optimization-1"></a><a class="docs-heading-anchor-permalink" href="#Box-Constrained-Optimization" title="Permalink"></a></h2><p>A primal interior-point algorithm for simple &quot;box&quot; constraints (lower and upper bounds) is available. Reusing our Rosenbrock example from above, boxed minimization is performed as follows:</p><pre><code class="language-jl hljs">lower = [1.25, -2.1]
upper = [Inf, Inf]
initial_x = [2.0, 2.0]
inner_optimizer = GradientDescent()
results = optimize(f, g!, lower, upper, initial_x, Fminbox(inner_optimizer))</code></pre><p>This performs optimization with a barrier penalty, successively scaling down the barrier coefficient and using the chosen <code>inner_optimizer</code> (<code>GradientDescent()</code> above) for convergence at each step. To change algorithm specific options, such as the line search algorithm, specify it directly in the <code>inner_optimizer</code> constructor:</p><pre><code class="nohighlight hljs">lower = [1.25, -2.1]
upper = [Inf, Inf]
initial_x = [2.0, 2.0]
# requires using LineSearches
inner_optimizer = GradientDescent(linesearch=LineSearches.BackTracking(order=3))
results = optimize(f, g!, lower, upper, initial_x, Fminbox(inner_optimizer))</code></pre><p>This algorithm uses diagonal preconditioning to improve the accuracy, and hence is a good example of how to use <code>ConjugateGradient</code> or <code>LBFGS</code> with preconditioning. Other methods will currently not use preconditioning. Only the box constraints are used. If you can analytically compute the diagonal of the Hessian of your objective function, you may want to consider writing your own preconditioner.</p><p>There are two iterations parameters: an outer iterations parameter used to control <code>Fminbox</code> and an inner iterations parameter used to control the inner optimizer. For example, the following restricts the optimization to 2 major iterations</p><pre><code class="language-julia hljs">results = optimize(f, g!, lower, upper, initial_x, Fminbox(GradientDescent()), Optim.Options(outer_iterations = 2))</code></pre><p>In contrast, the following sets the maximum number of iterations for each <code>GradientDescent()</code> optimization to 2</p><pre><code class="language-julia hljs">results = optimize(f, g!, lower, upper, initial_x, Fminbox(GradientDescent()), Optim.Options(iterations = 2))</code></pre><h3 id="Using-second-order-information"><a class="docs-heading-anchor" href="#Using-second-order-information">Using second order information</a><a id="Using-second-order-information-1"></a><a class="docs-heading-anchor-permalink" href="#Using-second-order-information" title="Permalink"></a></h3><p>When the Hessian of the objective function is available it is possible to use the primal-dual algorithm implemented in <code>IPNewton</code>. The interface is similar</p><pre><code class="language-julia hljs">results = optimize(f, lower, upper, initial_x, IPNewton())
results = optimize(f, g!, lower, upper, initial_x, IPNewton())
results = optimize(f, g!, h!, lower, upper, initial_x, IPNewton())</code></pre><h2 id="Minimizing-a-univariate-function-on-a-bounded-interval"><a class="docs-heading-anchor" href="#Minimizing-a-univariate-function-on-a-bounded-interval">Minimizing a univariate function on a bounded interval</a><a id="Minimizing-a-univariate-function-on-a-bounded-interval-1"></a><a class="docs-heading-anchor-permalink" href="#Minimizing-a-univariate-function-on-a-bounded-interval" title="Permalink"></a></h2><p>Minimization of univariate functions without derivatives is available through the <code>optimize</code> interface:</p><pre><code class="language-jl hljs">optimize(f, lower, upper, method; kwargs...)</code></pre><p>Notice the lack of initial <code>x</code>. A specific example is the following quadratic function.</p><pre><code class="language-jl hljs">julia&gt; f_univariate(x) = 2x^2+3x+1
f_univariate (generic function with 1 method)

julia&gt; optimize(f_univariate, -2.0, 1.0)
Results of Optimization Algorithm
 * Algorithm: Brent&#39;s Method
 * Search Interval: [-2.000000, 1.000000]
 * Minimizer: -7.500000e-01
 * Minimum: -1.250000e-01
 * Iterations: 7
 * Convergence: max(|x - x_upper|, |x - x_lower|) &lt;= 2*(1.5e-08*|x|+2.2e-16): true
 * Objective Function Calls: 8</code></pre><p>The output shows that we provided an initial lower and upper bound, that there is a final minimizer and minimum, and that it used seven major iterations. Importantly, we also see that convergence was declared. The default method is Brent&#39;s method, which is one out of two available methods:</p><ul><li>Brent&#39;s method, the default (can be explicitly selected with <code>Brent()</code>).</li><li>Golden section search, available with <code>GoldenSection()</code>.</li></ul><p>If we want to manually specify this method, we use the usual syntax as for multivariate optimization.</p><pre><code class="language-jl hljs">    optimize(f, lower, upper, Brent(); kwargs...)
    optimize(f, lower, upper, GoldenSection(); kwargs...)</code></pre><p>Keywords are used to set options for this special type of optimization. In addition to the <code>iterations</code>, <code>store_trace</code>, <code>show_trace</code>, <code>show_warnings</code>, and <code>extended_trace</code> options, the following options are also available:</p><ul><li><code>rel_tol</code>: The relative tolerance used for determining convergence. Defaults to <code>sqrt(eps(T))</code>.</li><li><code>abs_tol</code>: The absolute tolerance used for determining convergence. Defaults to <code>eps(T)</code>.</li></ul><h2 id="Obtaining-results"><a class="docs-heading-anchor" href="#Obtaining-results">Obtaining results</a><a id="Obtaining-results-1"></a><a class="docs-heading-anchor-permalink" href="#Obtaining-results" title="Permalink"></a></h2><p>After we have our results in <code>res</code>, we can use the API for getting optimization results. This consists of a collection of functions. They are not exported, so they have to be prefixed by <code>Optim.</code>. Say we do the following optimization:</p><pre><code class="language-jl hljs">res = optimize(x-&gt;dot(x,[1 0. 0; 0 3 0; 0 0 1]*x), zeros(3))</code></pre><p>If we can&#39;t remember what method we used, we simply use</p><pre><code class="language-jl hljs">summary(res)</code></pre><p>which will return <code>&quot;Nelder Mead&quot;</code>. A bit more useful information is the minimizer and minimum of the objective functions, which can be found using</p><pre><code class="language-jlcon hljs">julia&gt; Optim.minimizer(res)
3-element Array{Float64,1}:
 -0.499921
 -0.3333
 -1.49994

julia&gt; Optim.minimum(res)
 -2.8333333205768865</code></pre><h3 id="Complete-list-of-functions"><a class="docs-heading-anchor" href="#Complete-list-of-functions">Complete list of functions</a><a id="Complete-list-of-functions-1"></a><a class="docs-heading-anchor-permalink" href="#Complete-list-of-functions" title="Permalink"></a></h3><p>A complete list of functions can be found below.</p><p>Defined for all methods:</p><ul><li><code>summary(res)</code></li><li><code>minimizer(res)</code></li><li><code>minimum(res)</code></li><li><code>iterations(res)</code></li><li><code>iteration_limit_reached(res)</code></li><li><code>trace(res)</code></li><li><code>x_trace(res)</code></li><li><code>f_trace(res)</code></li><li><code>f_calls(res)</code></li><li><code>converged(res)</code></li></ul><p>Defined for univariate optimization:</p><ul><li><code>lower_bound(res)</code></li><li><code>upper_bound(res)</code></li><li><code>x_lower_trace(res)</code></li><li><code>x_upper_trace(res)</code></li><li><code>rel_tol(res)</code></li><li><code>abs_tol(res)</code></li></ul><p>Defined for multivariate optimization:</p><ul><li><code>g_norm_trace(res)</code></li><li><code>g_calls(res)</code></li><li><code>x_converged(res)</code></li><li><code>f_converged(res)</code></li><li><code>g_converged(res)</code></li><li><code>initial_state(res)</code></li></ul><p>Defined for <code>NelderMead</code> with the option <code>trace_simplex=true</code>:</p><ul><li><code>centroid_trace(res)</code> (with <code>extended_trace=true</code>)</li><li><code>simplex_trace(res)</code></li><li><code>simplex_values_trace(res)</code></li></ul><h2 id="Input-types"><a class="docs-heading-anchor" href="#Input-types">Input types</a><a id="Input-types-1"></a><a class="docs-heading-anchor-permalink" href="#Input-types" title="Permalink"></a></h2><p>Most users will input <code>Vector</code>&#39;s as their <code>initial_x</code>&#39;s, and get an <code>Optim.minimizer(res)</code> out that is also a vector. For zeroth and first order methods, it is also possible to pass in matrices, or even higher dimensional arrays. The only restriction imposed by leaving the <code>Vector</code> case is, that it is no longer possible to use finite difference approximations or automatic differentiation. Second order methods (variants of Newton&#39;s method) do not support this more general input type.</p><h2 id="Notes-on-convergence-flags-and-checks"><a class="docs-heading-anchor" href="#Notes-on-convergence-flags-and-checks">Notes on convergence flags and checks</a><a id="Notes-on-convergence-flags-and-checks-1"></a><a class="docs-heading-anchor-permalink" href="#Notes-on-convergence-flags-and-checks" title="Permalink"></a></h2><p>Currently, it is possible to access a minimizer using <code>Optim.minimizer(result)</code> even if all convergence flags are <code>false</code>. This means that the user has to be a bit careful when using the output from the solvers. It is advised to include checks for convergence if the minimizer or minimum is used to carry out further calculations.</p><p>A related note is that first and second order methods makes a convergence check on the gradient before entering the optimization loop. This is done to prevent line search errors if <code>initial_x</code> is a stationary point. Notice, that this is only a first order check. If <code>initial_x</code> is any type of stationary point, <code>g_converged</code> will be true. This includes local minima, saddle points, and local maxima. If <code>iterations</code> is <code>0</code> and <code>g_converged</code> is <code>true</code>, the user needs to keep this point in mind.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../">« Home</a><a class="docs-footer-nextpage" href="../gradientsandhessians/">Gradients and Hessians »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.12.0 on <span class="colophon-date" title="Thursday 12 June 2025 09:47">Thursday 12 June 2025</span>. Using Julia version 1.11.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
