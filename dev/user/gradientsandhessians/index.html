<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Gradients and Hessians · Optim</title><meta name="title" content="Gradients and Hessians · Optim"/><meta property="og:title" content="Gradients and Hessians · Optim"/><meta property="twitter:title" content="Gradients and Hessians · Optim"/><meta name="description" content="Documentation for Optim."/><meta property="og:description" content="Documentation for Optim."/><meta property="twitter:description" content="Documentation for Optim."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Optim</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../minimization/">Minimizing a function</a></li><li class="is-active"><a class="tocitem" href>Gradients and Hessians</a><ul class="internal"><li><a class="tocitem" href="#Gradients-and-Hessians"><span>Gradients and Hessians</span></a></li><li><a class="tocitem" href="#Analytic"><span>Analytic</span></a></li><li><a class="tocitem" href="#Finite-differences"><span>Finite differences</span></a></li><li><a class="tocitem" href="#Automatic-differentiation"><span>Automatic differentiation</span></a></li><li><a class="tocitem" href="#Example"><span>Example</span></a></li></ul></li><li><a class="tocitem" href="../config/">Configurable Options</a></li><li><a class="tocitem" href="../../algo/linesearch/">Linesearch</a></li><li><a class="tocitem" href="../algochoice/">Algorithm choice</a></li><li><a class="tocitem" href="../../algo/precondition/">Preconditioners</a></li><li><a class="tocitem" href="../../algo/complex/">Complex optimization</a></li><li><a class="tocitem" href="../../algo/manifolds/">Manifolds</a></li><li><a class="tocitem" href="../tipsandtricks/">Tips and tricks</a></li><li><a class="tocitem" href="../../examples/generated/ipnewton_basics/">Interior point Newton</a></li><li><a class="tocitem" href="../../examples/generated/maxlikenlm/">Maximum likelihood estimation</a></li><li><a class="tocitem" href="../../examples/generated/rasch/">Conditional maximum likelihood estimation</a></li></ul></li><li><span class="tocitem">Algorithms</span><ul><li><input class="collapse-toggle" id="menuitem-3-1" type="checkbox"/><label class="tocitem" for="menuitem-3-1"><span class="docs-label">Gradient Free</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../algo/nelder_mead/">Nelder Mead</a></li><li><a class="tocitem" href="../../algo/simulated_annealing/">Simulated Annealing</a></li><li><a class="tocitem" href="../../algo/samin/">Simulated Annealing w/ bounds</a></li><li><a class="tocitem" href="../../algo/particle_swarm/">Particle Swarm</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-3-2" type="checkbox"/><label class="tocitem" for="menuitem-3-2"><span class="docs-label">Gradient Required</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../algo/adam_adamax/">Adam and AdaMax</a></li><li><a class="tocitem" href="../../algo/cg/">Conjugate Gradient</a></li><li><a class="tocitem" href="../../algo/gradientdescent/">Gradient Descent</a></li><li><a class="tocitem" href="../../algo/lbfgs/">(L-)BFGS</a></li><li><a class="tocitem" href="../../algo/ngmres/">Acceleration</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-3-3" type="checkbox"/><label class="tocitem" for="menuitem-3-3"><span class="docs-label">Hessian Required</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../algo/newton/">Newton</a></li><li><a class="tocitem" href="../../algo/newton_trust_region/">Newton with Trust Region</a></li><li><a class="tocitem" href="../../algo/ipnewton/">Interior point Newton</a></li></ul></li></ul></li><li><a class="tocitem" href="../../dev/contributing/">Contributing</a></li><li><a class="tocitem" href="../../LICENSE/">License</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorials</a></li><li class="is-active"><a href>Gradients and Hessians</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Gradients and Hessians</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaNLSolvers/Optim.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JuliaNLSolvers/Optim.jl/blob/master/docs/src/user/gradientsandhessians.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h2 id="Gradients-and-Hessians"><a class="docs-heading-anchor" href="#Gradients-and-Hessians">Gradients and Hessians</a><a id="Gradients-and-Hessians-1"></a><a class="docs-heading-anchor-permalink" href="#Gradients-and-Hessians" title="Permalink"></a></h2><p>To use first- and second-order methods, you need to provide gradients and Hessians, either in-place or out-of-place. There are three main ways of specifying derivatives: analytic, finite-difference and automatic differentiation.</p><h2 id="Analytic"><a class="docs-heading-anchor" href="#Analytic">Analytic</a><a id="Analytic-1"></a><a class="docs-heading-anchor-permalink" href="#Analytic" title="Permalink"></a></h2><p>This results in the fastest run times, but requires the user to perform the often tedious task of computing the derivatives by hand. The gradient of complicated objective functions (e.g. involving the solution of algebraic equations, differential equations, eigendecompositions, etc.) can be computed efficiently using the adjoint method (see e.g. <a href="https://math.mit.edu/~stevenj/18.336/adjoint.pdf">these lecture notes</a>). In particular, assuming infinite memory, the gradient of a <span>$\mathbb{R}^N \to \mathbb{R}$</span> function <span>$f$</span> can always be computed with a runtime comparable with only one evaluation of <span>$f$</span>, no matter how large <span>$N$</span>.</p><p>To use analytic derivatives, simply pass <code>g!</code> and <code>h!</code> functions to <code>optimize</code>.</p><h2 id="Finite-differences"><a class="docs-heading-anchor" href="#Finite-differences">Finite differences</a><a id="Finite-differences-1"></a><a class="docs-heading-anchor-permalink" href="#Finite-differences" title="Permalink"></a></h2><p>This uses the functionality in <a href="https://github.com/JuliaDiffEq/DiffEqDiffTools.jl">DiffEqDiffTools.jl</a> to compute gradients and Hessians through central finite differences: <span>$f&#39;(x) \approx \frac{f(x+h)-f(x-h)}{2h}$</span>. For a <span>$\mathbb{R}^N \to \mathbb{R}$</span> objective function <span>$f$</span>, this requires <span>$2N$</span> evaluations of <span>$f$</span>. It is therefore efficient in low dimensions but slow when <span>$N$</span> is large. It is also inaccurate: <span>$h$</span> is chosen equal to <span>$\epsilon^{1/3}$</span> where <span>$\epsilon$</span> is the machine epsilon (about <span>$10^{-16}$</span> for <code>Float64</code>) to balance the truncation and rounding errors, resulting in an error of <span>$\epsilon^{2/3}$</span> (about <span>$10^{-11}$</span> for <code>Float64</code>) for the derivative.</p><p>Finite differences are on by default if gradients and Hessians are not supplied to the <code>optimize</code> call.</p><h2 id="Automatic-differentiation"><a class="docs-heading-anchor" href="#Automatic-differentiation">Automatic differentiation</a><a id="Automatic-differentiation-1"></a><a class="docs-heading-anchor-permalink" href="#Automatic-differentiation" title="Permalink"></a></h2><p>Automatic differentiation techniques are a middle ground between finite differences and analytic computations. They are exact up to machine precision, and do not require intervention from the user. They come in two main flavors: <a href="https://en.wikipedia.org/wiki/Automatic_differentiation">forward and reverse mode</a>. Forward-mode automatic differentiation is relatively straightforward to implement by propagating the sensitivities of the input variables, and is often faster than finite differences. The disadvantage is that the objective function has to be written using only Julia code. Forward-mode automatic differentiation still requires a runtime comparable to <span>$N$</span> evaluations of <span>$f$</span>, and is therefore costly in large dimensions, like finite differences.</p><p>Reverse-mode automatic differentiation can be seen as an automatic implementation of the adjoint method mentioned above, and requires a runtime comparable to only one evaluation of <span>$f$</span>. It is however considerably more complex to implement, requiring to record the execution of the program to then run it backwards, and incurs a larger overhead.</p><p>Forward-mode automatic differentiation is supported through the <a href="https://github.com/JuliaDiff/ForwardDiff.jl">ForwardDiff.jl</a> package by providing the <code>autodiff=:forward</code> keyword to <code>optimize</code>. More generic automatic differentiation is supported thanks to <a href="https://github.com/JuliaDiff/DifferentiationInterface.jl">DifferentiationInterface.jl</a>, by setting <code>autodiff</code> to any compatible backend object from <a href="https://github.com/SciML/ADTypes.jl">ADTypes.jl</a>. For instance, the user can choose <code>autodiff=AutoReverseDiff()</code>, <code>autodiff=AutoEnzyme()</code>, <code>autodiff=AutoMooncake()</code> or <code>autodiff=AutoZygote()</code> for a reverse-mode gradient computation, which is generally faster than forward mode on large inputs. Each of these choices requires loading the corresponding package beforehand.</p><h2 id="Example"><a class="docs-heading-anchor" href="#Example">Example</a><a id="Example-1"></a><a class="docs-heading-anchor-permalink" href="#Example" title="Permalink"></a></h2><p>Let us consider the Rosenbrock example again.</p><pre><code class="language-julia hljs">function f(x)
    return (1.0 - x[1])^2 + 100.0 * (x[2] - x[1]^2)^2
end

function g!(G, x)
    G[1] = -2.0 * (1.0 - x[1]) - 400.0 * (x[2] - x[1]^2) * x[1]
    G[2] = 200.0 * (x[2] - x[1]^2)
end

function h!(H, x)
    H[1, 1] = 2.0 - 400.0 * x[2] + 1200.0 * x[1]^2
    H[1, 2] = -400.0 * x[1]
    H[2, 1] = -400.0 * x[1]
    H[2, 2] = 200.0
end

initial_x = zeros(2)</code></pre><p>Let us see if BFGS and Newton&#39;s Method can solve this problem with the functions provided.</p><pre><code class="language-jlcon hljs">julia&gt; Optim.minimizer(optimize(f, g!, h!, initial_x, BFGS()))
2-element Array{Float64,1}:
 1.0
 1.0

julia&gt; Optim.minimizer(optimize(f, g!, h!, initial_x, Newton()))

2-element Array{Float64,1}:
 1.0
 1.0</code></pre><p>This is indeed the case. Now let us use finite differences for BFGS.</p><pre><code class="language-jlcon hljs">julia&gt; Optim.minimizer(optimize(f, initial_x, BFGS()))
2-element Array{Float64,1}:
 1.0
 1.0</code></pre><p>Still looks good. Returning to automatic differentiation, let us try both solvers using this method.  We enable <a href="https://github.com/JuliaDiff/ForwardDiff.jl">forward mode</a> automatic differentiation by using the <code>autodiff = :forward</code> keyword.</p><pre><code class="language-jlcon hljs">julia&gt; Optim.minimizer(optimize(f, initial_x, BFGS(); autodiff = :forward))
2-element Array{Float64,1}:
 1.0
 1.0

julia&gt; Optim.minimizer(optimize(f, initial_x, Newton(); autodiff = :forward))
2-element Array{Float64,1}:
 1.0
 1.0</code></pre><p>Indeed, the minimizer was found, without providing any gradients or Hessians.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../minimization/">« Minimizing a function</a><a class="docs-footer-nextpage" href="../config/">Configurable Options »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.15.0 on <span class="colophon-date" title="Saturday 1 November 2025 22:13">Saturday 1 November 2025</span>. Using Julia version 1.12.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
