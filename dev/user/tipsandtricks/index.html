<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Tips and tricks · Optim</title><meta name="title" content="Tips and tricks · Optim"/><meta property="og:title" content="Tips and tricks · Optim"/><meta property="twitter:title" content="Tips and tricks · Optim"/><meta name="description" content="Documentation for Optim."/><meta property="og:description" content="Documentation for Optim."/><meta property="twitter:description" content="Documentation for Optim."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Optim</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../minimization/">Minimizing a function</a></li><li><a class="tocitem" href="../gradientsandhessians/">Gradients and Hessians</a></li><li><a class="tocitem" href="../config/">Configurable Options</a></li><li><a class="tocitem" href="../../algo/linesearch/">Linesearch</a></li><li><a class="tocitem" href="../algochoice/">Algorithm choice</a></li><li><a class="tocitem" href="../../algo/precondition/">Preconditioners</a></li><li><a class="tocitem" href="../../algo/complex/">Complex optimization</a></li><li><a class="tocitem" href="../../algo/manifolds/">Manifolds</a></li><li class="is-active"><a class="tocitem" href>Tips and tricks</a><ul class="internal"><li><a class="tocitem" href="#Dealing-with-constant-parameters"><span>Dealing with constant parameters</span></a></li><li><a class="tocitem" href="#Avoid-repeating-computations"><span>Avoid repeating computations</span></a></li><li><a class="tocitem" href="#Provide-gradients"><span>Provide gradients</span></a></li><li><a class="tocitem" href="#Separating-time-spent-in-Optim&#39;s-code-and-user-provided-functions"><span>Separating time spent in Optim&#39;s code and user provided functions</span></a></li><li><a class="tocitem" href="#Early-stopping"><span>Early stopping</span></a></li></ul></li><li><a class="tocitem" href="../../examples/generated/ipnewton_basics/">Interior point Newton</a></li><li><a class="tocitem" href="../../examples/generated/maxlikenlm/">Maximum likelihood estimation</a></li><li><a class="tocitem" href="../../examples/generated/rasch/">Conditional maximum likelihood estimation</a></li></ul></li><li><span class="tocitem">Algorithms</span><ul><li><input class="collapse-toggle" id="menuitem-3-1" type="checkbox"/><label class="tocitem" for="menuitem-3-1"><span class="docs-label">Gradient Free</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../algo/nelder_mead/">Nelder Mead</a></li><li><a class="tocitem" href="../../algo/simulated_annealing/">Simulated Annealing</a></li><li><a class="tocitem" href="../../algo/samin/">Simulated Annealing w/ bounds</a></li><li><a class="tocitem" href="../../algo/particle_swarm/">Particle Swarm</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-3-2" type="checkbox"/><label class="tocitem" for="menuitem-3-2"><span class="docs-label">Gradient Required</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../algo/adam_adamax/">Adam and AdaMax</a></li><li><a class="tocitem" href="../../algo/cg/">Conjugate Gradient</a></li><li><a class="tocitem" href="../../algo/gradientdescent/">Gradient Descent</a></li><li><a class="tocitem" href="../../algo/lbfgs/">(L-)BFGS</a></li><li><a class="tocitem" href="../../algo/ngmres/">Acceleration</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-3-3" type="checkbox"/><label class="tocitem" for="menuitem-3-3"><span class="docs-label">Hessian Required</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../algo/newton/">Newton</a></li><li><a class="tocitem" href="../../algo/newton_trust_region/">Newton with Trust Region</a></li><li><a class="tocitem" href="../../algo/ipnewton/">Interior point Newton</a></li></ul></li></ul></li><li><a class="tocitem" href="../../dev/contributing/">Contributing</a></li><li><a class="tocitem" href="../../LICENSE/">License</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorials</a></li><li class="is-active"><a href>Tips and tricks</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Tips and tricks</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaNLSolvers/Optim.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JuliaNLSolvers/Optim.jl/blob/master/docs/src/user/tipsandtricks.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h2 id="Dealing-with-constant-parameters"><a class="docs-heading-anchor" href="#Dealing-with-constant-parameters">Dealing with constant parameters</a><a id="Dealing-with-constant-parameters-1"></a><a class="docs-heading-anchor-permalink" href="#Dealing-with-constant-parameters" title="Permalink"></a></h2><p>In many applications, there may be factors that are relevant to the function evaluations, but are fixed throughout the optimization. An obvious example is using data in a likelihood function, but it could also be parameters we wish to hold constant.</p><p>Consider a squared error loss function that depends on some data <code>x</code> and <code>y</code>, and parameters <code>betas</code>. As far as the solver is concerned, there should only be one input argument to the function we want to minimize, call it <code>sqerror</code>.</p><p>The problem is that we want to optimize a function <code>sqerror</code> that really depends on three inputs, and two of them are constant throughout the optimization procedure. To do this, we need to define the variables <code>x</code> and <code>y</code></p><pre><code class="language-jl hljs">x = [1.0, 2.0, 3.0]
y = 1.0 .+ 2.0 .* x .+ [-0.3, 0.3, -0.1]</code></pre><p>We then simply define a function in three variables</p><pre><code class="language-julia hljs">function sqerror(betas, X, Y)
    err = 0.0
    for i in 1:length(X)
        pred_i = betas[1] + betas[2] * X[i]
        err += (Y[i] - pred_i)^2
    end
    return err
end</code></pre><p>and then optimize the following anonymous function</p><pre><code class="language-jl hljs">res = optimize(b -&gt; sqerror(b, x, y), [0.0, 0.0])</code></pre><p>Alternatively, we can define a closure <code>sqerror(betas)</code> that is aware of the variables we just defined</p><pre><code class="language-jl hljs">function sqerror(betas)
    err = 0.0
    for i in 1:length(x)
        pred_i = betas[1] + betas[2] * x[i]
        err += (y[i] - pred_i)^2
    end
    return err
end</code></pre><p>We can then optimize the <code>sqerror</code> function just like any other function</p><pre><code class="language-jl hljs">res = optimize(sqerror, [0.0, 0.0])</code></pre><h2 id="Avoid-repeating-computations"><a class="docs-heading-anchor" href="#Avoid-repeating-computations">Avoid repeating computations</a><a id="Avoid-repeating-computations-1"></a><a class="docs-heading-anchor-permalink" href="#Avoid-repeating-computations" title="Permalink"></a></h2><p>Say you are optimizing a function</p><pre><code class="language-julia hljs">f(x) = x[1]^2+x[2]^2
g!(storage, x) = copyto!(storage, [2x[1], 2x[2]])</code></pre><p>In this situation, no calculations from <code>f</code> could be reused in <code>g!</code>. However, sometimes there is a substantial similarity between the objective function, and gradient, and some calculations can be reused.</p><p>To avoid repeating calculations, define functions <code>fg!</code> or <code>fgh!</code> that compute the objective function, the gradient and the Hessian (if needed) simultaneously. These functions internally can be written to avoid repeating common calculations.</p><p>For example, here we define a function <code>fg!</code> to compute the objective function and the gradient, as required:</p><pre><code class="language-julia hljs">function fg!(F, G, x)
  # do common computations here
  # ...
  if G !== nothing
    # code to compute gradient here
    # writing the result to the vector G
    # G .= ...
  end
  if F !== nothing
    # value = ... code to compute objective function
    return value
  end
end</code></pre><p><code>Optim</code> will only call this function with an argument <code>G</code> that is <code>nothing</code> (if the gradient is not required) or a <code>Vector</code> that should be filled (in-place) with the gradient. This flexibility is convenient for algorithms that only use the gradient in some iterations but not in others.</p><p>Now we call <code>optimize</code> with the following syntax:</p><pre><code class="language-julia hljs">Optim.optimize(Optim.only_fg!(fg!), [0., 0.], Optim.LBFGS())</code></pre><p>Similarly, for a computation that requires the Hessian, we can write:</p><pre><code class="language-julia hljs">function fgh!(F, G, H, x)
  G === nothing || # compute gradient and store in G
  H === nothing || # compute Hessian and store in H
  F === nothing || return f(x)
  nothing
end

Optim.optimize(Optim.only_fgh!(fgh!), [0., 0.], Optim.Newton())</code></pre><h2 id="Provide-gradients"><a class="docs-heading-anchor" href="#Provide-gradients">Provide gradients</a><a id="Provide-gradients-1"></a><a class="docs-heading-anchor-permalink" href="#Provide-gradients" title="Permalink"></a></h2><p>As mentioned in the general introduction, passing analytical gradients can have an impact on performance. To show an example of this, consider the separable extension of the Rosenbrock function in dimension 5000, see <a href="ftp://ftp.numerical.rl.ac.uk/pub/cutest/sif/SROSENBR.SIF">SROSENBR</a> in CUTEst.</p><p>Below, we use the gradients and objective functions from <a href="http://www.cuter.rl.ac.uk/Problems/mastsif.shtml">mastsif</a> through <a href="https://github.com/JuliaSmoothOptimizers/CUTEst.jl">CUTEst.jl</a>. We only show the first five iterations of an attempt to minimize the function using Gradient Descent.</p><pre><code class="language-jlcon hljs">julia&gt; @time optimize(f, initial_x, GradientDescent(),
                      Optim.Options(show_trace=true, iterations = 5))
Iter     Function value   Gradient norm
     0     4.850000e+04     2.116000e+02
     1     1.018734e+03     2.704951e+01
     2     3.468449e+00     5.721261e-01
     3     2.966899e+00     2.638790e-02
     4     2.511859e+00     5.237768e-01
     5     2.107853e+00     1.020287e-01
 21.731129 seconds (1.61 M allocations: 63.434 MB, 0.03% gc time)
Results of Optimization Algorithm
 * Algorithm: Gradient Descent
 * Starting Point: [1.2,1.0, ...]
 * Minimizer: [1.0287767703731154,1.058769439356144, ...]
 * Minimum: 2.107853e+00
 * Iterations: 5
 * Convergence: false
   * |x - x&#39;| &lt; 0.0: false
   * |f(x) - f(x&#39;)| / |f(x)| &lt; 0.0: false
   * |g(x)| &lt; 1.0e-08: false
   * Reached Maximum Number of Iterations: true
 * Objective Function Calls: 23
 * Gradient Calls: 23

julia&gt; @time optimize(f, g!, initial_x, GradientDescent(),
                      Optim.Options(show_trace=true, iterations = 5))
Iter     Function value   Gradient norm
     0     4.850000e+04     2.116000e+02
     1     1.018769e+03     2.704998e+01
     2     3.468488e+00     5.721481e-01
     3     2.966900e+00     2.638792e-02
     4     2.511828e+00     5.237919e-01
     5     2.107802e+00     1.020415e-01
  0.009889 seconds (915 allocations: 270.266 KB)
Results of Optimization Algorithm
 * Algorithm: Gradient Descent
 * Starting Point: [1.2,1.0, ...]
 * Minimizer: [1.0287763814102757,1.05876866832087, ...]
 * Minimum: 2.107802e+00
 * Iterations: 5
 * Convergence: false
   * |x - x&#39;| &lt; 0.0: false
   * |f(x) - f(x&#39;)| / |f(x)| &lt; 0.0: false
   * |g(x)| &lt; 1.0e-08: false
   * Reached Maximum Number of Iterations: true
 * Objective Function Calls: 23
 * Gradient Calls: 23</code></pre><p>The objective has obtained a value that is very similar between the two runs, but the run with the analytical gradient is way faster.  It is possible that the finite differences code can be improved, but generally the optimization will be slowed down by all the function evaluations required to do the central finite differences calculations.</p><h2 id="Separating-time-spent-in-Optim&#39;s-code-and-user-provided-functions"><a class="docs-heading-anchor" href="#Separating-time-spent-in-Optim&#39;s-code-and-user-provided-functions">Separating time spent in Optim&#39;s code and user provided functions</a><a id="Separating-time-spent-in-Optim&#39;s-code-and-user-provided-functions-1"></a><a class="docs-heading-anchor-permalink" href="#Separating-time-spent-in-Optim&#39;s-code-and-user-provided-functions" title="Permalink"></a></h2><p>Consider the Rosenbrock problem.</p><pre><code class="language-julia hljs">using Optim, OptimTestProblems
prob = UnconstrainedProblems.examples[&quot;Rosenbrock&quot;];</code></pre><p>Say we optimize this function, and look at the total run time of <code>optimize</code> using the Newton Trust Region method, and we are surprised that it takes a long time to run. We then wonder if time is spent in Optim&#39;s own code (solving the sub-problem for example) or in evaluating the objective, gradient or hessian that we provided. Then it can be very useful to use the <a href="https://github.com/KristofferC/TimerOutputs.jl">TimerOutputs.jl</a> package. This package allows us to run an over-all timer for <code>optimize</code>, and add individual timers for <code>f</code>, <code>g!</code>, and <code>h!</code>. Consider the example below, that is due to the author of the package (Kristoffer Carlsson).</p><pre><code class="language-julia hljs">using TimerOutputs
const to = TimerOutput()

f(x    ) =  @timeit to &quot;f&quot;  prob.f(x)
g!(x, g) =  @timeit to &quot;g!&quot; prob.g!(x, g)
h!(x, h) =  @timeit to &quot;h!&quot; prob.h!(x, h)

begin
reset_timer!(to)
@timeit to &quot;Trust Region&quot; begin
    res = Optim.optimize(f, g!, h!, prob.initial_x, NewtonTrustRegion())
end
show(to; allocations = false)
end</code></pre><p>We see that the time is actually <em>not</em> spent in our provided functions, but most of the time is spent in the code for the trust region method.</p><h2 id="Early-stopping"><a class="docs-heading-anchor" href="#Early-stopping">Early stopping</a><a id="Early-stopping-1"></a><a class="docs-heading-anchor-permalink" href="#Early-stopping" title="Permalink"></a></h2><p>Sometimes it might be of interest to stop the optimizer early. The simplest way to do this is to set the <code>iterations</code> keyword in <code>Optim.Options</code> to some number. This will prevent the iteration counter exceeding some limit, with the standard value being 1000. Alternatively, it is possible to put a soft limit on the run time of the optimization procedure by setting the <code>time_limit</code> keyword in the <code>Optim.Options</code> constructor.</p><pre><code class="language-julia hljs">using Optim, OptimTestProblems
problem = UnconstrainedProblems.examples[&quot;Rosenbrock&quot;]

f = problem.f
initial_x = problem.initial_x

function slow(x)
    sleep(0.1)
    f(x)
end

start_time = time()

optimize(slow, zeros(2), NelderMead(), Optim.Options(time_limit = 3.0))</code></pre><p>This will stop after about three seconds. If it is more important that we stop before the limit is reached, it is possible to use a callback with a simple model for predicting how much time will have passed when the next iteration is over. Consider the following code</p><pre><code class="language-julia hljs">using Optim, OptimTestProblems
problem = UnconstrainedProblems.examples[&quot;Rosenbrock&quot;]

f = problem.f
initial_x = problem.initial_x

function very_slow(x)
    sleep(.5)
    f(x)
end

start_time = time()
time_to_setup = zeros(1)
function advanced_time_control(x)
    println(&quot; * Iteration:       &quot;, x.iteration)
    so_far =  time()-start_time
    println(&quot; * Time so far:     &quot;, so_far)
    if x.iteration == 0
        time_to_setup .= time()-start_time
    else
        expected_next_time = so_far + (time()-start_time-time_to_setup[1])/(x.iteration)
        println(&quot; * Next iteration ≈ &quot;, expected_next_time)
        println()
        return expected_next_time &lt; 13 ? false : true
    end
    println()
    false
end
optimize(very_slow, zeros(2), NelderMead(), Optim.Options(callback = advanced_time_control))</code></pre><p>It will try to predict the elapsed time after the next iteration is over, and stop now if it is expected to exceed the limit of 13 seconds. Running it, we get something like the following output</p><pre><code class="language-jlcon hljs">julia&gt; optimize(very_slow, zeros(2), NelderMead(), Optim.Options(callback = advanced_time_control))
 * Iteration:       0
 * Time so far:     2.219298839569092

 * Iteration:       1
 * Time so far:     3.4006409645080566
 * Next iteration ≈ 4.5429909229278564

 * Iteration:       2
 * Time so far:     4.403923988342285
 * Next iteration ≈ 5.476739525794983

 * Iteration:       3
 * Time so far:     5.407265901565552
 * Next iteration ≈ 6.4569235642751055

 * Iteration:       4
 * Time so far:     5.909044027328491
 * Next iteration ≈ 6.821732044219971

 * Iteration:       5
 * Time so far:     6.912338972091675
 * Next iteration ≈ 7.843148183822632

 * Iteration:       6
 * Time so far:     7.9156060218811035
 * Next iteration ≈ 8.85849153995514

 * Iteration:       7
 * Time so far:     8.918903827667236
 * Next iteration ≈ 9.870419979095459

 * Iteration:       8
 * Time so far:     9.922197818756104
 * Next iteration ≈ 10.880185931921005

 * Iteration:       9
 * Time so far:     10.925468921661377
 * Next iteration ≈ 11.888488478130764

 * Iteration:       10
 * Time so far:     11.92870283126831
 * Next iteration ≈ 12.895747828483582

 * Iteration:       11
 * Time so far:     12.932114839553833
 * Next iteration ≈ 13.902462200684981

Results of Optimization Algorithm
 * Algorithm: Nelder-Mead
 * Starting Point: [0.0,0.0]
 * Minimizer: [0.23359374999999996,0.042187499999999996, ...]
 * Minimum: 6.291677e-01
 * Iterations: 11
 * Convergence: false
   *  √(Σ(yᵢ-ȳ)²)/n &lt; 1.0e-08: false
   * Reached Maximum Number of Iterations: false
 * Objective Function Calls: 24</code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../algo/manifolds/">« Manifolds</a><a class="docs-footer-nextpage" href="../../examples/generated/ipnewton_basics/">Interior point Newton »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.7.0 on <span class="colophon-date" title="Tuesday 29 October 2024 08:26">Tuesday 29 October 2024</span>. Using Julia version 1.11.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
