<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <meta name="author" content="JuliaNLSolvers">
  <link rel="shortcut icon" href="../../img/favicon.ico">
  <title>Acceleration - Optim.jl</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="../../css/highlight.css">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Acceleration";
    var mkdocs_page_input_path = "algo/ngmres.md";
    var mkdocs_page_url = "/algo/ngmres/";
  </script>
  
  <script src="../../js/jquery-2.1.1.min.js"></script>
  <script src="../../js/modernizr-2.8.3.min.js"></script>
  <script type="text/javascript" src="../../js/highlight.pack.js"></script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href="../.." class="icon icon-home"> Optim.jl</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
    <a class="" href="../..">Home</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">General information</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../user/minimization/">Minimizing a function</a>
                </li>
                <li class="">
                    
    <a class="" href="../../user/config/">Configurable Options</a>
                </li>
                <li class="">
                    
    <a class="" href="../../user/tipsandtricks/">Tips and tricks</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Algorithms</span>
    <ul class="subnav">
                <li class=" current">
                    
    <span class="caption-text">Solvers</span>
    <ul class="subnav">
                <li class="toctree-l3">
                    
    <span class="caption-text">Gradient Free</span>
    <ul class="subnav">
                <li class="toctree-l4">
                    
    <a class="" href="../nelder_mead/">Nelder Mead</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../simulated_annealing/">Simulated Annealing</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../particle_swarm/">Particle Swarm</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3 current">
                    
    <span class="caption-text">Gradient Required</span>
    <ul class="subnav">
                <li class="toctree-l4">
                    
    <a class="" href="../cg/">Conjugate Gradient</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../gradientdescent/">Gradient Descent</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../lbfgs/">(L-)BFGS</a>
                </li>
                <li class="toctree-l4 current">
                    
    <a class="current" href="./">Acceleration</a>
    <ul class="subnav">
            
    <li class="toctree-l5"><a href="#acceleration-methods-n-gmres-and-o-accel">Acceleration methods: N-GMRES and O-ACCEL</a></li>
    
        <ul>
        
            <li><a class="toctree-l6" href="#constructors">Constructors</a></li>
        
            <li><a class="toctree-l6" href="#description">Description</a></li>
        
            <li><a class="toctree-l6" href="#example">Example</a></li>
        
            <li><a class="toctree-l6" href="#references">References</a></li>
        
        </ul>
    

    </ul>
                </li>
    </ul>
                </li>
                <li class="toctree-l3">
                    
    <span class="caption-text">Hessian Required</span>
    <ul class="subnav">
                <li class="toctree-l4">
                    
    <a class="" href="../newton/">Newton</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../newton_trust_region/">Newton with Trust Region</a>
                </li>
    </ul>
                </li>
    </ul>
                </li>
                <li class="">
                    
    <a class="" href="../autodiff/">Automatic Differentiation</a>
                </li>
                <li class="">
                    
    <a class="" href="../linesearch/">Linesearch</a>
                </li>
                <li class="">
                    
    <a class="" href="../precondition/">Preconditioners</a>
                </li>
                <li class="">
                    
    <a class="" href="../complex/">Complex optimization</a>
                </li>
                <li class="">
                    
    <a class="" href="../manifolds/">Manifolds</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Contributing</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../dev/contributing/">Contributing</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../LICENSE/">License</a>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../..">Optim.jl</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../..">Docs</a> &raquo;</li>
    
      
        
          <li>Solvers &raquo;</li>
        
      
        
          <li>Gradient Required &raquo;</li>
        
      
    
    <li>Acceleration</li>
    <li class="wy-breadcrumbs-aside">
      
        <a href="https://github.com/JuliaNLSolvers/Optim.jl/edit/master/docs/algo/ngmres.md"
          class="icon icon-github"> Edit on GitHub</a>
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <p><a id='Acceleration-methods:-N-GMRES-and-O-ACCEL-1'></a></p>
<h1 id="acceleration-methods-n-gmres-and-o-accel">Acceleration methods: N-GMRES and O-ACCEL</h1>
<p><a id='Constructors-1'></a></p>
<h2 id="constructors">Constructors</h2>
<div class="codehilite"><pre><span></span><span class="n">NGMRES</span><span class="p">(;</span>
        <span class="n">alphaguess</span> <span class="o">=</span> <span class="n">LineSearches</span><span class="o">.</span><span class="n">InitialStatic</span><span class="p">(),</span>
        <span class="n">linesearch</span> <span class="o">=</span> <span class="n">LineSearches</span><span class="o">.</span><span class="n">HagerZhang</span><span class="p">(),</span>
        <span class="n">manifold</span> <span class="o">=</span> <span class="n">Flat</span><span class="p">(),</span>
        <span class="n">wmax</span><span class="o">::</span><span class="kt">Int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
        <span class="n">ϵ0</span> <span class="o">=</span> <span class="mf">1e-12</span><span class="p">,</span>
        <span class="n">nlprecon</span> <span class="o">=</span> <span class="n">GradientDescent</span><span class="p">(</span>
            <span class="n">alphaguess</span> <span class="o">=</span> <span class="n">LineSearches</span><span class="o">.</span><span class="n">InitialPrevious</span><span class="p">(),</span>
            <span class="n">linesearch</span> <span class="o">=</span> <span class="n">LineSearches</span><span class="o">.</span><span class="n">Static</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span><span class="n">scaled</span><span class="o">=</span><span class="kc">true</span><span class="p">),</span>
            <span class="n">manifold</span> <span class="o">=</span> <span class="n">manifold</span><span class="p">),</span>
        <span class="n">nlpreconopts</span> <span class="o">=</span> <span class="n">Options</span><span class="p">(</span><span class="n">iterations</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">allow_f_increases</span> <span class="o">=</span> <span class="kc">true</span><span class="p">),</span>
      <span class="p">)</span>
</pre></div>


<div class="codehilite"><pre><span></span><span class="n">OACCEL</span><span class="p">(;</span><span class="n">manifold</span><span class="o">::</span><span class="n">Manifold</span> <span class="o">=</span> <span class="n">Flat</span><span class="p">(),</span>
       <span class="n">alphaguess</span> <span class="o">=</span> <span class="n">LineSearches</span><span class="o">.</span><span class="n">InitialStatic</span><span class="p">(),</span>
       <span class="n">linesearch</span> <span class="o">=</span> <span class="n">LineSearches</span><span class="o">.</span><span class="n">HagerZhang</span><span class="p">(),</span>
       <span class="n">nlprecon</span> <span class="o">=</span> <span class="n">GradientDescent</span><span class="p">(</span>
           <span class="n">alphaguess</span> <span class="o">=</span> <span class="n">LineSearches</span><span class="o">.</span><span class="n">InitialPrevious</span><span class="p">(),</span>
           <span class="n">linesearch</span> <span class="o">=</span> <span class="n">LineSearches</span><span class="o">.</span><span class="n">Static</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span><span class="n">scaled</span><span class="o">=</span><span class="kc">true</span><span class="p">),</span>
           <span class="n">manifold</span> <span class="o">=</span> <span class="n">manifold</span><span class="p">),</span>
       <span class="n">nlpreconopts</span> <span class="o">=</span> <span class="n">Options</span><span class="p">(</span><span class="n">iterations</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">allow_f_increases</span> <span class="o">=</span> <span class="kc">true</span><span class="p">),</span>
       <span class="n">ϵ0</span> <span class="o">=</span> <span class="mf">1e-12</span><span class="p">,</span>
       <span class="n">wmax</span><span class="o">::</span><span class="kt">Int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">)</span>
</pre></div>


<p><a id='Description-1'></a></p>
<h2 id="description">Description</h2>
<p>These algorithms take a step given by the nonlinear preconditioner <code>nlprecon</code> and proposes an accelerated step on a subspace spanned by the previous <code>wmax</code> iterates.</p>
<ul>
<li>N-GMRES accelerates based on a minimization of an approximation to the $\ell_2$ norm of the</li>
</ul>
<p>gradient.</p>
<ul>
<li>O-ACCEL accelerates based on a minimization of a n approximation to the objective.</li>
</ul>
<p>N-GMRES was originally developed for solving nonlinear systems [1], and reduces to GMRES for linear problems. Application of the algorithm to optimization is covered, for example, in [2]. A description of O-ACCEL and its connection to N-GMRES can be found in [3].</p>
<p><em>We recommend trying <a href="../lbfgs/">LBFGS</a> on your problem before N-GMRES or O-ACCEL. All three algorithms have similar computational cost and memory requirements, however, L-BFGS is more efficient for many problems.</em></p>
<p><a id='Example-1'></a></p>
<h2 id="example">Example</h2>
<p>This example shows how to accelerate <code>GradientDescent</code> on the Extended Rosenbrock problem. First, we try to optimize using <code>GradientDescent</code>.</p>
<div class="codehilite"><pre><span></span><span class="k">using</span> <span class="n">Optim</span><span class="p">,</span> <span class="n">OptimTestProblems</span>
<span class="n">UP</span> <span class="o">=</span> <span class="n">OptimTestProblems</span><span class="o">.</span><span class="n">UnconstrainedProblems</span>
<span class="n">prob</span> <span class="o">=</span> <span class="n">UP</span><span class="o">.</span><span class="n">examples</span><span class="p">[</span><span class="s">&quot;Extended Rosenbrock&quot;</span><span class="p">]</span>
<span class="n">optimize</span><span class="p">(</span><span class="n">UP</span><span class="o">.</span><span class="n">objective</span><span class="p">(</span><span class="n">prob</span><span class="p">),</span> <span class="n">UP</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">prob</span><span class="p">),</span> <span class="n">prob</span><span class="o">.</span><span class="n">initial_x</span><span class="p">,</span> <span class="n">GradientDescent</span><span class="p">())</span>
</pre></div>


<p>The algorithm does not converge within 1000 iterations.</p>
<div class="codehilite"><pre><span></span>Results of Optimization Algorithm
 * Algorithm: Gradient Descent
 * Starting Point: [-1.2,1.0, ...]
 * Minimizer: [0.8923389282461412,0.7961268644300445, ...]
 * Minimum: 2.898230e-01
 * Iterations: 1000
 * Convergence: false
   * |x - x&#39;| ≤ 1.0e-32: false 
     |x - x&#39;| = 4.02e-04 
   * |f(x) - f(x&#39;)| ≤ 1.0e-32 |f(x)|: false
     |f(x) - f(x&#39;)| = 2.38e-03 |f(x)|
   * |g(x)| ≤ 1.0e-08: false 
     |g(x)| = 8.23e-02 
   * Stopped by an increasing objective: false
   * Reached Maximum Number of Iterations: true
 * Objective Calls: 2525
 * Gradient Calls: 2525
</pre></div>


<p>Now, we use <code>OACCEL</code> to accelerate <code>GradientDescent</code>.</p>
<div class="codehilite"><pre><span></span><span class="c"># Default nonlinear procenditioner for `OACCEL`</span>
<span class="n">nlprecon</span> <span class="o">=</span>  <span class="n">GradientDescent</span><span class="p">(</span><span class="n">linesearch</span><span class="o">=</span><span class="n">LineSearches</span><span class="o">.</span><span class="n">Static</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span><span class="n">scaled</span><span class="o">=</span><span class="kc">true</span><span class="p">))</span>
<span class="c"># Default size of subspace that OACCEL accelerates over is `wmax = 10`</span>
<span class="n">oacc10</span> <span class="o">=</span> <span class="n">OACCEL</span><span class="p">(</span><span class="n">nlprecon</span><span class="o">=</span><span class="n">nlprecon</span><span class="p">,</span> <span class="n">wmax</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">optimize</span><span class="p">(</span><span class="n">UP</span><span class="o">.</span><span class="n">objective</span><span class="p">(</span><span class="n">prob</span><span class="p">),</span> <span class="n">UP</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">prob</span><span class="p">),</span> <span class="n">prob</span><span class="o">.</span><span class="n">initial_x</span><span class="p">,</span> <span class="n">oacc10</span><span class="p">)</span>
</pre></div>


<p>This drastically improves the <code>GradientDescent</code> algorithm, converging in 87 iterations.</p>
<div class="codehilite"><pre><span></span>Results of Optimization Algorithm
 * Algorithm: O-ACCEL preconditioned with Gradient Descent
 * Starting Point: [-1.2,1.0, ...]
 * Minimizer: [1.0000000011361219,1.0000000022828495, ...]
 * Minimum: 3.255053e-17
 * Iterations: 87
 * Convergence: true
   * |x - x&#39;| ≤ 1.0e-32: false 
     |x - x&#39;| = 6.51e-08 
   * |f(x) - f(x&#39;)| ≤ 1.0e-32 |f(x)|: false
     |f(x) - f(x&#39;)| = 7.56e+02 |f(x)|
   * |g(x)| ≤ 1.0e-08: true 
     |g(x)| = 1.06e-09 
   * Stopped by an increasing objective: false
   * Reached Maximum Number of Iterations: false
 * Objective Calls: 285
 * Gradient Calls: 285
</pre></div>


<p>We can improve the acceleration further by changing the acceleration subspace size <code>wmax</code>.</p>
<div class="codehilite"><pre><span></span><span class="n">oacc5</span> <span class="o">=</span> <span class="n">OACCEL</span><span class="p">(</span><span class="n">nlprecon</span><span class="o">=</span><span class="n">nlprecon</span><span class="p">,</span> <span class="n">wmax</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">optimize</span><span class="p">(</span><span class="n">UP</span><span class="o">.</span><span class="n">objective</span><span class="p">(</span><span class="n">prob</span><span class="p">),</span> <span class="n">UP</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">prob</span><span class="p">),</span> <span class="n">prob</span><span class="o">.</span><span class="n">initial_x</span><span class="p">,</span> <span class="n">oacc5</span><span class="p">)</span>
</pre></div>


<p>Now, the O-ACCEL algorithm has accelerated <code>GradientDescent</code> to converge in 50 iterations.</p>
<div class="codehilite"><pre><span></span>Results of Optimization Algorithm
 * Algorithm: O-ACCEL preconditioned with Gradient Descent
 * Starting Point: [-1.2,1.0, ...]
 * Minimizer: [0.9999999999392858,0.9999999998784691, ...]
 * Minimum: 9.218164e-20
 * Iterations: 50
 * Convergence: true
   * |x - x&#39;| ≤ 1.0e-32: false 
     |x - x&#39;| = 2.76e-07 
   * |f(x) - f(x&#39;)| ≤ 1.0e-32 |f(x)|: false
     |f(x) - f(x&#39;)| = 5.18e+06 |f(x)|
   * |g(x)| ≤ 1.0e-08: true 
     |g(x)| = 4.02e-11 
   * Stopped by an increasing objective: false
   * Reached Maximum Number of Iterations: false
 * Objective Calls: 181
 * Gradient Calls: 181
</pre></div>


<p>As a final comparison, we can do the same with N-GMRES.</p>
<div class="codehilite"><pre><span></span><span class="n">ngmres5</span> <span class="o">=</span> <span class="n">NGMRES</span><span class="p">(</span><span class="n">nlprecon</span><span class="o">=</span><span class="n">nlprecon</span><span class="p">,</span> <span class="n">wmax</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">optimize</span><span class="p">(</span><span class="n">UP</span><span class="o">.</span><span class="n">objective</span><span class="p">(</span><span class="n">prob</span><span class="p">),</span> <span class="n">UP</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">prob</span><span class="p">),</span> <span class="n">prob</span><span class="o">.</span><span class="n">initial_x</span><span class="p">,</span> <span class="n">ngmres5</span><span class="p">)</span>
</pre></div>


<p>Again, this significantly improves the <code>GradientDescent</code> algorithm, and converges in 63 iterations.</p>
<div class="codehilite"><pre><span></span>Results of Optimization Algorithm
 * Algorithm: Nonlinear GMRES preconditioned with Gradient Descent
 * Starting Point: [-1.2,1.0, ...]
 * Minimizer: [0.9999999998534468,0.9999999997063993, ...]
 * Minimum: 5.375569e-19
 * Iterations: 63
 * Convergence: true
   * |x - x&#39;| ≤ 1.0e-32: false 
     |x - x&#39;| = 9.94e-09 
   * |f(x) - f(x&#39;)| ≤ 1.0e-32 |f(x)|: false
     |f(x) - f(x&#39;)| = 1.29e+03 |f(x)|
   * |g(x)| ≤ 1.0e-08: true 
     |g(x)| = 4.94e-11 
   * Stopped by an increasing objective: false
   * Reached Maximum Number of Iterations: false
 * Objective Calls: 222
 * Gradient Calls: 222
</pre></div>


<p><a id='References-1'></a></p>
<h2 id="references">References</h2>
<p>[1] De Sterck. Steepest descent preconditioning for nonlinear GMRES optimization. NLAA, 2013. [2] Washio and Oosterlee. Krylov subspace acceleration for nonlinear multigrid schemes. ETNA, 1997. [3] Riseth. Objective acceleration for unconstrained optimization. 2018.</p>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../newton/" class="btn btn-neutral float-right" title="Newton">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../lbfgs/" class="btn btn-neutral" title="(L-)BFGS"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
          <a href="https://github.com/JuliaNLSolvers/Optim.jl/" class="fa fa-github" style="float: left; color: #fcfcfc"> GitHub</a>
      
      
        <span><a href="../lbfgs/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../newton/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '../..';</script>
    <script src="../../js/theme.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML"></script>
      <script src="../../assets/mathjaxhelper.js"></script>
      <script src="../../search/require.js"></script>
      <script src="../../search/search.js"></script>

</body>
</html>
